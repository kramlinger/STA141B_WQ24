{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff704263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA 141B Data & Web Technologies for Data Analysis\n",
    "\n",
    "### Lecture 15, 2/5/24, Natural language processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Today's topics\n",
    "- Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b92d3",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a probabilistic model for a collection of discrete data. It is motivated to analyise texts. The discrete data consiststs on *words*, that are contain in *documents*, which in turn are collected in the *corpus*. Each document contains differents *topics*, which we are interested in. The topics are not observable, i.e., latent, and have to be estimated. \n",
    "\n",
    "- *word* is the smallest entity (token, term) of the discrete data and an element from the dictionary of $D$ words. The $i$-th word of the dictionary is a vector with zero entries except on position $i$. \n",
    "- *documents* are a series of $N$ words, denoted as $W = (w_1, \\dots, w_N)$. \n",
    "- *corpus* is the collection of $M$ documents, $\\{W_1, \\dots, W_M\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3d981",
   "metadata": {},
   "source": [
    "### References\n",
    " - [Latent Dirichlet Analysis](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151dd16",
   "metadata": {},
   "source": [
    "A LDA is modelled via a Bayesian hierarchy. Before we state it, we have to restate some distributions. \n",
    "\n",
    "##### Multinomial\n",
    "\n",
    "Let $n,k\\in\\{0,1,2,...\\}$ and $p = (p_1, \\dots, p_k)'\\in[0,1]^k$ with $\\sum_{i=1}^kp_i=1$. $X\\sim Mult(n,p)$ if \n",
    "$$\n",
    "P(X_1=x_1, \\dots, X_k = x_k) = \\begin{cases} \n",
    "\\frac{n!}{x_1\\cdots x_k!}p_1^{x_1}\\cdots p_k^{x_k}& \\text{if}\\sum_{i=1}^kx_i = n,\\\\\n",
    "0, &\\text{else.}\n",
    "\\end{cases}\n",
    "$$\n",
    "The Multinomial distribution is a generalization of the Binomial distribution for the case of more than two results. \n",
    "\n",
    "##### Dirichlet \n",
    "\n",
    "The Dirichlet distribution is a generalization of the Beta distribution. \n",
    "A Dirichlet distribution of order $k\\geq2$ and parameters $\\alpha_1, \\dots, \\alpha_k>0$ has the p.d.f. \n",
    "$$\n",
    "f(p_1,\\dots, p_k) = \\frac{\\Gamma(\\sum_{i=1}^k\\alpha_i)}{\\prod_{i=1}^k\\Gamma(\\alpha_i)}\\prod_{i=1}^kp_i^{\\alpha_i-1}\n",
    "$$\n",
    "for all $p_1, \\dots, p_k\\geq0$ with $\\sum_{i=1}^kp_i=1$. This implies that the $p_i$ lie on a $k-1$-dimensional simplex. For $k=2$, the Dirichlet distribution coincides with the Beta distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5376236",
   "metadata": {},
   "source": [
    "An important propery of the Dirichlet distribution is that its a conjugate prior to the Multiomial distribution. Consider the following hierarchy: \n",
    "\\begin{align}\n",
    "X|p&\\sim Mult(n,p)\\\\\n",
    "p&\\sim Dir(\\alpha)\n",
    "\\end{align}\n",
    "The parameter $p$ is not assumed to be fixed (as in an frequentist understanding) but random. Consequently, it cannot be estimated. The Bayesian methodology aims in computing the posterior distribution of $p$, given the observed data $X$:\n",
    "\\begin{align}\n",
    "P(p|X) = \\frac{P(X|p)P(p)}{P(X)}\n",
    "\\end{align}\n",
    "One can show that the posterior probabiliy is again a Dirichlet distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad42be",
   "metadata": {},
   "source": [
    "Back to the data! We assume that every document is a collection of latent topics. Each topic is determined by the distribution of words in the document. For each document, we assume the data generating process: \n",
    "\n",
    "- Choose $\\theta\\sim Dir(\\alpha)$, where $\\theta, \\alpha\\in\\mathbb{R}^k$. \n",
    "- For each word $w_i$ in the document $W$, $i = 1, \\dots, N$: \n",
    "    - Choose a topic $z_i\\sim Mult(1,\\theta)$,\n",
    "    - Choose a word from $P(w_i|z_i,\\beta)$. \n",
    "        \n",
    "Here, $P(w_i|z_i,\\beta)$ is a multinomial probability with $n=1$, given topic $z_i$. We assume that the number of topics $k$ is known and will not be modelled as random for now. The parameter $\\beta\\in\\mathbb{R}^{k\\times D}$, and $\\beta_{ij}$ is the probability that word $j$ is chosen for topic $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a0862",
   "metadata": {},
   "source": [
    "##### Example\n",
    "\n",
    "Consider as corpus a magazine about housekeeping. This magazine consists of three articles (documents), and each article consists of the topics `home`, `garden`, `cooking` and the words `pan`, `plot`, `window` and `way`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e6fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c567e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = np.array(['home', 'garden', 'cooking'])\n",
    "dictionary = np.array(['pan', 'tree', 'window',  'way'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2b275",
   "metadata": {},
   "source": [
    "For parameters $\\alpha_i$, the parameter $\\theta$ determines the probability of topics in each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e87ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [1, 2, 3]\n",
    "theta = np.random.dirichlet(alpha)\n",
    "theta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72293cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "zi = np.random.multinomial(1, theta) # topic for word \n",
    "zi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d53056",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.array([[0.1, 0.02, 0.88, 0],[0.01, 0.79, 0.1, 0.1],[0.75, 0.15, 0.1, 0]])\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta[zi==1,][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.multinomial(1,beta[zi==1,][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee341f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "zi = np.random.multinomial(1, theta) # topic for word \n",
    "wi = np.random.multinomial(1, beta[zi==1,][0]) \n",
    "\n",
    "print({'topic': topics[zi==1][0]})\n",
    "print({'word': dictionary[wi==1][0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763db8b0",
   "metadata": {},
   "source": [
    "The complete corresponding hierarchy is given as \n",
    "\\begin{align}\n",
    "    w_i|z_i,\\beta_i&\\sim Mult(1,\\beta_i)\\\\\n",
    "    z_i|\\theta &\\sim Mult(1,\\theta)\\\\\n",
    "    \\theta&\\sim Dir(\\alpha)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d8390",
   "metadata": {},
   "source": [
    "If $\\beta_i$ and $\\alpha$ are known, the posterior probability of $\\theta, z_i|w_i, \\alpha, \\beta_i$ could be approximated using MCMC methods. The method of empirical Bayes uses estimates of $\\beta_i$ and $\\alpha$ to do so:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9227928",
   "metadata": {},
   "source": [
    "Note that the joint distribution for a single word is given by \n",
    "$$P(\\theta, z_i, w_i|\\alpha_i, \\beta_i) = P(w_i| z_i, \\beta_i)P(z_i, w_i|\\theta)P(\\theta|\\alpha), $$\n",
    "and for a single document (let $Z = (z_1, \\dots, z_N)$) thus by \n",
    "$$P(\\theta, Z,W|\\alpha, \\beta) = \\prod_{i=1}^N P(w_i| z_i, \\beta_i) = P(\\theta|\\alpha) \\prod_{i=1}^N P(w_i| z_i, \\beta_i)P(z_i, w_i|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50131aa4",
   "metadata": {},
   "source": [
    "Integrating over $\\theta$ and summing over $Z$ gives the marginal distribution over this document: \n",
    "$$\n",
    "P(W|\\alpha, \\beta) \n",
    "= \n",
    "\\int P(\\theta|\\alpha) \\prod_{i=1}^N \\sum_{z_i}P(w_i| z_i, \\beta_i)P(z_i, w_i|\\theta) d\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40124e7c",
   "metadata": {},
   "source": [
    "The marginal distribution for the corpus $C$ is thus given as \n",
    "$$\n",
    "P(C|\\alpha, \\beta) \n",
    "= \n",
    "\\prod_{j=1}^M \\int P(\\theta|\\alpha) \\prod_{i=1}^N \\sum_{z_{ij}}P(w_i| z_{ij}, \\beta_i)P(z_{ij}, w_{ij}|\\theta) d\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78df3e1",
   "metadata": {},
   "source": [
    "Natural estimators for $\\alpha$ and $\\beta$ are found by solving \n",
    "$$\n",
    "\\max_{\\alpha, \\beta} P(C|\\alpha, \\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226f3fb",
   "metadata": {},
   "source": [
    "![LDAnormal](../images/LDAnormal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66098b7",
   "metadata": {},
   "source": [
    "Problematically, if the vocabulary is large enough, it is likely of encountering a new word. Using EB, those words would receive probability zero. The usual approach in such cases is to smooth the underlying probability distribution for the words: \n",
    "\n",
    "\\begin{align}\n",
    "    w_i|z_i,\\beta_i&\\sim Mult(1,\\beta_i)\\\\\n",
    "    \\beta_i|\\eta &\\sim Dir(\\eta)\\\\\n",
    "    z_i|\\theta &\\sim Mult(1,\\theta)\\\\\n",
    "    \\theta&\\sim Dir(\\alpha)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2eb5c",
   "metadata": {},
   "source": [
    "The hyperparameters $\\alpha$ and $\\eta$ are estimated using EB as outlined above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef582344",
   "metadata": {},
   "source": [
    "![LDAnormal](../images/LDAsmooth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e1fa6",
   "metadata": {},
   "source": [
    "#### [&#128011;](https://www.gutenberg.org/files/2701/2701-h/2701-h.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cedd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby = nltk.corpus.gutenberg.raw(\"melville-moby_dick.txt\")\n",
    "pattern = r\"(?<!,\\s{1})(?:ETYMOLOGY|CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS(?=\\s*\\())(?:\\.*\\s*).+?\\s*.*[\\.{1}|!{1}|?{1}|\\){1}]\"\n",
    "corpus = re.split(pattern, moby)\n",
    "corpus.pop(0)\n",
    "corpus = [re.sub(r\"\\s+\", \" \", document).lower() for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0c934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus[106][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68b6df",
   "metadata": {},
   "source": [
    "From [here](https://digitalcommons.cwu.edu/cgi/viewcontent.cgi?article=1430&context=etd#:~:text=In%20Chapters%201%2D8%2C%2010,role%20of%20%22I%22%20narrator.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cetological = [0, 1, 46, 54, 59, 60, 61, 63, 64, 68, 75, 76, 77, 78, 80, 81, 85, 86, 87, 89, 90, 91, 93, 96, 97, 98, 102, 103, 104, 105, 106, 107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = [i not in cetological for i in range(138)]\n",
    "labels = [i for i in map(lambda x: 'story' if x else 'ceto', story)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[130:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc725249",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = [nltk.word_tokenize(document) for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b95986",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65aa6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords.extend([',', '.', ':', '!', ';', '?', '--', '\\'', '\\'\\''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc1123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = [[word for word in document if word not in stopwords] for document in corpus_tokenized] # remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044d5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = [[nltk.PorterStemmer().stem(word) for word in document] for document in corpus_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [' '.join(document) for document in corpus_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[2][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae304cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec84b9d",
   "metadata": {},
   "source": [
    "Now, we will use the [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). \n",
    "\n",
    "We will have to reshape our corpus into a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(tokenizer = nltk.word_tokenize)\n",
    "freq = vec.fit_transform(corpus)\n",
    "corpus_freq = freq.todense() # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_freq = np.array(corpus_freq) # removes warning further down\n",
    "corpus_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc3b3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_freq.shape # some words less than when checked last time, due to stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4875a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics = 2\n",
    "lda = LatentDirichletAllocation(n_components = 2, \n",
    "                                learning_method = 'online', \n",
    "                                random_state = 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d19e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(corpus_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ed3a9",
   "metadata": {},
   "source": [
    "Now what? Lets investigate if the topic distribution changes over the chapters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e8337",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = lda.transform(corpus_freq)\n",
    "posterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(posterior[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474de03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(posterior).reset_index()\n",
    "df.columns = ['chapter'] + ['Topic ' + str(i + 1) for i in range(0,ntopics)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.melt(df, id_vars = 'chapter')\n",
    "ddf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bdf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.line(ddf, x=\"chapter\", y=\"value\", color='variable', labels={\n",
    "                     \"chapter\": \"Chapter\",\n",
    "                     \"value\": \"Probability\",\n",
    "                     \"variable\": \"LDA Topics\"\n",
    "                 }, title = 'LDA Probabilities: Labelled cetology chapters are shaded')\n",
    "\n",
    "for i, e in enumerate(labels): \n",
    "    if e=='ceto': \n",
    "        fig.add_vrect(x0=i-0.5, x1=i+0.5, line_width=0, fillcolor=\"grey\", opacity=0.2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5de617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[(df['Topic 1']>0.335).values] # chapters of topic 1 - change cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lda.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e29568",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTopics = pd.DataFrame(lda.components_.T, index = vec.get_feature_names_out())\n",
    "wordTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTopics = wordTopics.apply(lambda x: x / sum(x), 1)\n",
    "wordTopics.columns = ['Topic ' + str(i + 1) for i in range(0,ntopics)]\n",
    "wordTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTopics['Topic 1'].sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61b683",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordTopics['Topic 2'].sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ffcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2b12ec50b0a525a62abe739e766b0c808eccd181e3f804cedbbca00f4d5b392"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
