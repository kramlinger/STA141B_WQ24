{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff704263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA 141B Data & Web Technologies for Data Analysis\n",
    "\n",
    "### Lecture 12, 2/20/24, Natural language processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9297f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Last Week's topics\n",
    "- Web Scraping: \n",
    "    - Foodwise\n",
    "    - Tornado Watch\n",
    "    - Ratemyprofessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Today's topics\n",
    "- Natural Language Processing\n",
    "     - `nltk` package\n",
    "     - Tokenization\n",
    "     - Regular Expressions\n",
    "     - Standardizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f66f37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ressources\n",
    "- [Natural Language Processing with Python][nlpp], chapters 1-3. Beware: the print version is for Python 2.\n",
    "- [Scikit-Learn Documentation][skl], especially the section about [Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "\n",
    "[PDSH]: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "[ProGit]: https://git-scm.com/book/\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "[atap]: https://search.library.ucdavis.edu/primo-explore/fulldisplay?docid=01UCD_ALMA51320822340003126&context=L&vid=01UCD_V1&search_scope=everything_scope&tab=default_tab&lang=en_US\n",
    "[skl]: https://scikit-learn.org/stable/documentation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2af2f1",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "\n",
    "A _natural language_ is a language people use to communicate, like English, Spanish, or Mandarin. These languages evolved over thousands of years and do not have simple, explicit rules.\n",
    "\n",
    "_Natural language processing_ (NLP) means using a computer to analyze, manipulate, or synthesize natural language. Some examples of NLP tasks are:\n",
    "* Translating from one language to another\n",
    "* Recognizing speech or handwriting\n",
    "* Tagging sentences with metadata, such as parts of speech (verbs, nouns, etc) or sentiment\n",
    "* Extracting information or computing statistics from text\n",
    "\n",
    "Compared to artificial languages like Python and XML, it's much more difficult to extract information from natural languages. NLP is a wide field; we only have time to learn the absolute basics. If you want to learn more, consider reading the entire [Natural Language Processing with Python][nlpp] book or taking a class in computational linguistics.\n",
    "\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "\n",
    "\n",
    "#### The Python NLP Ecosystem\n",
    "\n",
    "There are lots of Python packages for NLP (try searching online)! A few popular ones are:\n",
    "\n",
    "* [Natural Language Tool Kit][nltk] (`nltk`) is the most popular. It's designed for learning and research, so it's well-documented and has lots of features. We will use `nltk` for this class. \n",
    "* [TextBlob][textblob] is a \"simplified\" package. It has a nicer interface than NLTK, but less features.\n",
    "* [SpaCy][spacy] is a \"production-ready\" package, and the fastest of all the packages listed here. Useful for working with large natural language datasets.\n",
    "* [gensim][gensim] is a package for creating topic models, which are a kind of statistical model that predict the topics of a text.\n",
    "\n",
    "We're going to learn `nltk`, but you might want to try some of the others if your project involves NLP.\n",
    "\n",
    "[Stanford's Core NLP][CoreNLP] library is at the cutting edge of NLP research. It's developed in Java, but several Python packages provide an interface (such as [pynlp][] and [stanford-corenlp][]).\n",
    "\n",
    "[nltk]: https://www.nltk.org/\n",
    "[spacy]: https://spacy.io/\n",
    "[textblob]: https://textblob.readthedocs.io/en/dev/\n",
    "[gensim]: https://radimrehurek.com/gensim/\n",
    "[CoreNLP]: https://stanfordnlp.github.io/CoreNLP/\n",
    "[pynlp]: https://github.com/sina-al/pynlp\n",
    "[stanford-corenlp]: https://github.com/Lynten/stanford-corenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26945e94",
   "metadata": {},
   "source": [
    "#### Corpora and Documents\n",
    "\n",
    "A _document_ is a single body text. When working with natural language data, documents are the unit of observation.\n",
    "\n",
    "What you choose as a document depends on the purpose of your analysis. If you're studying how people react to news on Twitter, it makes sense to use individual tweets as documents. If you're studying how animals are portrayed in 19th-century literature, you could use individual novels as documents.\n",
    "\n",
    "A _corpus_ is a collection of documents. In other words, a corpus is a dataset.\n",
    "\n",
    "`nltk` provides some example corpora in the `nltk.corpus` submodule. The documentation gives a [complete list](http://www.nltk.org/nltk_data/). Most have to be downloaded with `nltk.download()` before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2032bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "\n",
    "# Download books from Project Gutenberg\n",
    "nltk.download(\"gutenberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b660f8",
   "metadata": {},
   "source": [
    "The `.fileids()` method lists the documents in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39931181",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86920f",
   "metadata": {},
   "source": [
    "Lets talk about [whales](https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2H_4_0002). The `.raw()` method returns the raw text for a single document. Specify the document by its file ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby = nltk.corpus.gutenberg.raw(\"melville-moby_dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby[0:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c5740",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "A _token_ is a sequence of characters to be treated as a group. Tokens are the unit of analysis for an indvidual document.\n",
    "\n",
    "Tokens can represent paragraphs, sentences, words, or something else. Most of the time, tokens will be words.\n",
    "\n",
    "When you analyze a document, the first step will usually be to split the document into tokens. Functions that do this are called _tokenizers_, and this process is called _tokenization_.\n",
    "\n",
    "The `nltk.sent_tokenize()` function splits a document into sentences, and the `nltk.word_tokenize()` function splits a document into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nltk.sent_tokenize(moby))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.sent_tokenize(moby)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cd3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.sent_tokenize(moby)[283]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.word_tokenize(moby)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6572807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f49ada67",
   "metadata": {},
   "source": [
    "Corpora also have `.sents()` and `.word()` methods for tokenization. These methods are specialized to the corpus, so they sometimes use the different strategies than `sent_tokenize()` and `word_tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = nltk.corpus.gutenberg.sents(\"melville-moby_dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79083f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.gutenberg.words(\"melville-moby_dick.txt\")[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97988f44",
   "metadata": {},
   "source": [
    "### Strings and String Methods\n",
    "\n",
    "Lets continue talking about \t&#128011;. How does word tokenization actually work? The simplest strategy is to split at whitespace. You can do this with Python's built-in string methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10654bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby.split() # splits on whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80036a65",
   "metadata": {},
   "source": [
    "Splitting on whitespace doesn't handle punctuation. You can use regular expressions to split on more complex patterns. Python's built-in `re` module provides regular expression functions [here](https://docs.python.org/3/library/re.html).\n",
    "\n",
    "```\n",
    "re.split(pattern, string, maxsplit=0, flags=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"[ ,.:;!?()']\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e59b4d",
   "metadata": {},
   "source": [
    "What if we also want to split at newlines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b36f30",
   "metadata": {},
   "source": [
    "### Escape Sequences and Raw Strings\n",
    "\n",
    "In Python strings, backslash `\\` marks the beginning of an _escape sequence_. Escape sequences are special codes for writing characters that you can't otherwise type. For example, `\\n` is a new line character and `\\t` is a tab character.\n",
    "\n",
    "Since `\\` has a special meaning in strings, to write a literal `\\` you must use the escape sequence `\\\\`.\n",
    "\n",
    "You can see the actual characters in a string by printing the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b462ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\\nworld.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40378afa",
   "metadata": {},
   "source": [
    "The regular expression (Regex) language is independent of Python and also uses backslash `\\` to mark the beginning of an escape sequence. Regex escape sequences disable special behavior for characters. For example, `.` matches any character, but `\\.` only matches a literal `.`.\n",
    "\n",
    "As a result, writing a regular expression in an ordinary Python string is awkward. For example, to match a literal `\\`, we need to write `\\\\` in regular expressions, which is `\\\\\\\\` in an ordinary Python string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9125a",
   "metadata": {},
   "source": [
    "Python provides _raw strings_, where `\\` has no special meaning for Python, to help solve this problem. You can create a raw string by putting an `r` before the starting quote:\n",
    "\n",
    "More about raw strings: [here](https://www.journaldev.com/23598/python-raw-string#:~:text=Python%20raw%20string%20is%20created,treated%20as%20an%20escape%20character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"\\ \") # print(r\"\\\") returns an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978292ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r\"\\\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r'\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Hi\\nHello'\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0514bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_s = r'Hi\\nHello'\n",
    "raw_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd7b22",
   "metadata": {},
   "source": [
    "Even raw strings can't end in `\\;` this is a limitation of the Python parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60232c18",
   "metadata": {},
   "source": [
    "Now we can write a better regular expression to split with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e7e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(re.split(r\"[ \\[\\](),.:;!?'\\n\\r]\", moby)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac155f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"\\s\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49226b32",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a004f0",
   "metadata": {},
   "source": [
    "The regular expressions language includes _character classes_ that describe common sets of characters. The whitespace class `\\s` and the word class `\\w` are useful here (see [Reference](https://docs.python.org/3/library/re.html)). So to split on any whitespace character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ca632",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = r'[ ,.:;!\\n\\r]'\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea51be4",
   "metadata": {},
   "source": [
    "In a raw string, `re.split` looks for regex escapes; in a non-raw string, the function looks for the literal ASCII character. If these coincide, the string does not have to be converted to a raw string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b06e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.split(\"[ \\[\\],.:;!'()\\n]\", moby) # note the '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78462934",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"[ \\[\\],.:;!'()\\n]\", moby) # note the '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560d94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"[ \\[\\],.:;!'()\\n]\", moby) # note the '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959067e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"\\s\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea3263",
   "metadata": {},
   "source": [
    "Capitalizing a character classes inverts the meaning, so to split on all non-word characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b045b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"\\W+\", moby) # + matches 1 or more of the preceding characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a87887",
   "metadata": {},
   "source": [
    "`\\w` means _any word character_\n",
    "\n",
    "`+` Causes the resulting RE to match 1 or more repetitions of the preceding RE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b716f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"\\W+\", \"the...dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38002b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"\\W+\", \"the,dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc616c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"\\W+\", \"the,I:dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba07991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"\\W+\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418a22a1",
   "metadata": {},
   "source": [
    "Rather than splitting the text, you can also approach the problem from the perspective of extracting tokens. The `findall()` function returns all matches for a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a86938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re.findall(r\"\\w+\", \"The dog barked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d73b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\w\") # \\w is not a special python escape sequence, so it passes through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fcdd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r\"\\W+\", \"The dog barked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc4505",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"\\w+'?\\w{1}\", \"The dog's toy barked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"\\w+'?\\w{1}!?\", \"The dog's toy barked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f2cbf",
   "metadata": {},
   "source": [
    "- `r\" \"`: read the string\n",
    "- `()+`: the patterns inside the parathesis should appear once or more\n",
    "- `\\w+`: the whole word\n",
    "- `|`: or\n",
    "\n",
    "More practice? [here](https://regex101.com/?fbclid=IwAR36UyAxywvpSvTOh7F-KYI72IZAVQ0wRcBc0OEOu6h4MifEf-iLcFedfyk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c785cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.findall(r\"\\w+\", moby)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ebc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102beb47",
   "metadata": {},
   "source": [
    "Lets try to match all chapters in the book. First, lets match the chapter sequence, they are similar to \"\\nCHAPTER 1\\r\\n\\r\\nLoomings.\\r\\n\". Check the novel [here](https://www.gutenberg.org/files/2701/2701-h/2701-h.htm#link2H_4_0002). Note that the full stop after the chapter is not in the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"CHAPTER\\s{1}\\d+\\s*\\w+\\.{1}\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f72b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(CHAPTER\\s{1}\\d+)\\s*(\\w+\\.{1})\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04eaacd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "re.findall(r\"(CHAPTER\\s{1}\\d+)\\s*(.+\\.{1})\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1edf6f",
   "metadata": {},
   "source": [
    "See chapter 43. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(CHAPTER\\s{1}\\d+)\\s*(.+[\\.{1}|!{1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b9ccd",
   "metadata": {},
   "source": [
    "Chapter 1 reappeared! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<=,)\\s*(CHAPTER\\s{1}\\d+)\\s*(.+[\\.{1}|!{1}])\", moby) # do not capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f84f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1a670eb",
   "metadata": {},
   "source": [
    "Lets use a negatve lookbehind! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a2ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s*)(CHAPTER\\s{1}\\d+)\\s*(.+[\\.{1}|!{1}])\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54df124",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(CHAPTER\\s{1}\\d+)\\s*(.+[\\.{1}|!{1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294deef",
   "metadata": {},
   "source": [
    "Lets find the unmatched chapters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chapters = [i for i in range(1,135)]\n",
    "matched_chapters = [int(i) for i in \n",
    "                    re.findall(r\"(?<!,\\s{1})(?:CHAPTER\\s{1})(\\d+)(?:\\s*.+[\\.{1}|!{1}])\", moby)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a2547",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in all_chapters if not i in matched_chapters ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137c3bf",
   "metadata": {},
   "source": [
    "There is another new line! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(CHAPTER\\s{1}\\d+)\\s*(.+\\s*.*[\\.{1}|!{1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366c53c",
   "metadata": {},
   "source": [
    "Lets be lazy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac815fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(CHAPTER\\s{1}\\d+)\\s*(.+?\\s*.*[\\.{1}|!{1}])\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS)\\s*(.+?\\s*.*[\\.{1}|!{1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ba69a",
   "metadata": {},
   "source": [
    "Lets use a positive lookahead `(?=...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fa3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS(?=\\s*\\())\\s*(.+?\\s*.*[\\.{1}|!{1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e23be5",
   "metadata": {},
   "source": [
    "To match `\"ETYMOLOGY.\"`, we have to account for parenthesis. (Note the extra `\\.*`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"(?<!,\\s{1})(ETYMOLOGY|CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS(?=\\s*\\())\\.*\\s*(.+?\\s*.*[\\.{1}|!{1}|\\){1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5e3cd",
   "metadata": {},
   "source": [
    "Perfect! But what if we want to match the chapters that follow after his matched string? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"((?<!,\\s{1})(?:ETYMOLOGY|CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS(?=\\s*\\())(?:\\.*\\s*).+?\\s*.*[\\.{1}|!{1}|\\){1}])\", moby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64843203",
   "metadata": {},
   "source": [
    "Check the [docs](https://docs.python.org/3/library/re.html#re.split). Remove the capturing group when splitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbeb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = re.split(r\"(?<!,\\s{1})(?:ETYMOLOGY|CHAPTER\\s{1}\\d+|Epilogue|EXTRACTS(?=\\s*\\())(?:\\.*\\s*).+?\\s*.*[\\.{1}|!{1}|\\){1}]\", moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c146dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = [re.sub(r\"\\s+\", \" \", chapter) for chapter in chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter = chapters[3]\n",
    "chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f294cd7",
   "metadata": {},
   "source": [
    "Back to tokenizing! Tokenizing natural languages is a difficult problem. Some tokenizers work better for certain kinds of documents than others.\n",
    "\n",
    "Before building your own tokenizer, try the tokenizers included with __nltk__, in the `nltk.tokenize` submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f94d8",
   "metadata": {},
   "source": [
    "### Standardizing Text\n",
    "\n",
    "We standardize numerical data in order to make fair comparisons, comparisons that are not influenced by the location and scale of the data. Similarly, you can standardize text (tokens) to make sure comparisons are fair and accurate.\n",
    "\n",
    "For example, `\"Cat\"` and `\"cat\"` are the same word even though they're different tokens. Converting all characters to lowercase is one way to standardize a document.\n",
    "\n",
    "Some common standardization techniques for text are:\n",
    "\n",
    "* Lowercasing\n",
    "* Stemming: Use patterns to remove prefixes and suffixes from words.\n",
    "* Lemmatiziation: Look up each token in a dictionary and replace it with a root word. Similar to stemming, but more accurate.\n",
    "* Stopword Removal: Remove tokens that don't contribute meaning. For example, \"the\" is meaningless on its own.\n",
    "* Identifying Outliers: Identify and possibly remove non-standard \"words\" like numbers, mispellings, code, etc...\n",
    "\n",
    "How and whether you should standardize a document or corpus depends on what kind of analysis you want to do. There is no formula; you must think carefully and experiment to determine which standardization techniques work best for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92498eb3",
   "metadata": {},
   "source": [
    "#### Lowercasing\n",
    "\n",
    "You can use Python's string methods for simple text transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc71198",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter.lower()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f02f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter.upper()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b650aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.findall(r\"\\w+\", chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb972c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = [w.lower() for w in words] # lower and upper\n",
    "lower[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85ffc3",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "_Stemming_ runs an algorithm on each token to remove affixes (prefixes and suffixes). The result is called a _stem_.\n",
    "\n",
    "Stemming is useful if you want to ignore affixes.\n",
    "\n",
    "For example, most English verbs use suffixes to mark the tense. We write \"They fish\" (present) and \"They fished\" (past). Without any standardization, the tokens \"fish\" and \"fished\" would be treated as separate words. Stemming converts both tokens to the common stem \"fish\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5fb85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[nltk.PorterStemmer().stem(w) for w in words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78092c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.PorterStemmer().stem(\"whales\"))\n",
    "print(nltk.PorterStemmer().stem(\"whaling\"))\n",
    "print(nltk.PorterStemmer().stem(\"whalebone\"))\n",
    "print(nltk.PorterStemmer().stem(\"narwhales\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74f0e6",
   "metadata": {},
   "source": [
    "Stemmers use a sequence of rules to determine the stem for each token, but natural languages are full of special cases and exceptions. So as you can see in the example above, some stems are not words , and sometimes tokens that seem like they should have the same stem don't.\n",
    "\n",
    "Several different stemmers are provided in the `nltk.stem` submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc949c",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "_Lemmatization_ looks up each token in a dictionary to find a root word, or _lemma_.\n",
    "\n",
    "Lemmatization serves the same purpose as stemming. Lemmatization is more accurate, but requires a dictionary and usually takes longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize(\"whales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize(\"whaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17885c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize(\"whaling\", \"v\") #this is a verb - it should be lemmatized to 'whale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc390be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize(\"whalebone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28eced",
   "metadata": {},
   "source": [
    "The WordNet lemmatizer requires part of speech information in order to lemmatize words. You can get approximate part of speech information with __nltk__'s `pos_tag()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0997f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag([\"whaling\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae88f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag([\"whale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdce05",
   "metadata": {},
   "source": [
    "NLTK POS Tags are [Brown POS tags][brown]\n",
    "\n",
    "[brown]: https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a6f9d",
   "metadata": {},
   "source": [
    "#### Foreign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = SnowballStemmer('french')\n",
    "\n",
    "sent = \"En mathématiques, une fonction càdlàg (continue à droite, limite à gauche) est ...\"\n",
    "nltk.word_tokenize(sent)\n",
    "\n",
    "nltk.pos_tag([fr.stem(word) for word in nltk.word_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be58789",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_tags = nltk.pos_tag(words)\n",
    "moby_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd0c9f",
   "metadata": {},
   "source": [
    "The `nltk.stem` submodule also provides several different lemmatizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62368977",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "_Stopwords_ are words that appear frequently but don't add meaning.\n",
    "\n",
    "In English, \"the\", \"a\", and \"at\" are examples. However, exactly which words are stopwords depends on your analysis. Words that are meaningless in one analysis might be very important in others.\n",
    "\n",
    "You can filter out stopwords with a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"the\", \"a\", \"and\", \"or\", \"in\", \"by\"]\n",
    "[w for w in words if w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8ef17",
   "metadata": {},
   "source": [
    "__nltk__ also provides a stopwords corpus that contains common stopwords for several languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "[w for w in words if w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c164b",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "- Learn Regular Expressions to rule natural languages \n",
    "- Processing depends on use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2b12ec50b0a525a62abe739e766b0c808eccd181e3f804cedbbca00f4d5b392"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
