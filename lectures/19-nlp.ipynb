{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff704263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA 141B Data & Web Technologies for Data Analysis\n",
    "\n",
    "### Lecture 19, 3/14/24, Natural language processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4068f8",
   "metadata": {},
   "source": [
    "### Announcements \n",
    "\n",
    "- Extra OH this Friday, 3/15, 12-1 PM via Zoom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Today's topics\n",
    "- Chunking\n",
    "    - Noun Phrase Chunking\n",
    "    - Tag Patterns\n",
    "    - Developing and Evaluating Chunkers\n",
    "    - Chinking\n",
    "- Training Classifier-Based Chunkers\n",
    "- Cascaded Chunker\n",
    "- Named Entity Recognition\n",
    "- Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bed30e",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import lxml.html as lx\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17579e",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def get_info(name):\n",
    "    time.sleep(0.2)\n",
    "    name = name.lower()\n",
    "    name = re.sub(' ', '-', name)\n",
    "    name = re.sub('[^\\w-]', '', name)\n",
    "    result = requests.get('https://www.cia.gov/the-world-factbook/page-data/countries/' \\\n",
    "                          + name + '/page-data.json')\n",
    "    result.raise_for_status()\n",
    "    return result.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c201c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get('https://www.cia.gov/the-world-factbook/page-data/sq/d/1627106492.json')\n",
    "country_names = [i.get('name') \\\n",
    "                 if i.get('redirect') is None else i.get('redirect').get('name') \\\n",
    "                 for i in result.json()['data']['countries']['nodes']]\n",
    "countries = [get_info(name)['result']['data'] for name in country_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad220b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [i for i, e in enumerate(countries) if e['country']['name'] == \"Burma\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd6a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [i for i in countries[index]['fields']['nodes'] if i.get('name') == 'Background'][0]['data']\n",
    "document = \"\".join([t for t in lx.fromstring(document).xpath('//p/text()')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ecc03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document):\n",
    "    document = document.lower()\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164728e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_document = preprocess(document)\n",
    "sentence = processed_document[0]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ed049",
   "metadata": {},
   "source": [
    "#### Noun Phrase Chunking\n",
    "\n",
    "We will begin by considering the task of noun phrase chunking, or NP-chunking,\n",
    "where we search for chunks corresponding to individual noun phrases.\n",
    "\n",
    "One of the most useful sources of information for NP-chunking is part-of-speech tags. This is one of the motivations for performing part-of-speech tagging in our information extraction system. In order to create an NP-chunker, we will first define a chunk grammar, consisting of rules that indicate how sentences should be chunked.\n",
    "\n",
    "Ee will define a simple grammar with a single regular expression rule . This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). Using this grammar, we create a chunk parser, and test it on our example sentence . The result is a tree, which we can either print, or display graphically ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b7bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN.*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4aaeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cp.parse(sentence)\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45eb7b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5559486",
   "metadata": {},
   "source": [
    "#### Tag Patterns\n",
    "\n",
    "The rules that make up a chunk grammar use tag patterns to describe sequences of\n",
    "tagged words. A tag pattern is a sequence of part-of-speech tags delimited using angle\n",
    "brackets, e.g.,`<DT>?<JJ>*<NN>`. Tag patterns are similar to regular expression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15809737",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ.*>*<NN.*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdaeb33",
   "metadata": {},
   "source": [
    "This will chunk any sequence of tokens beginning with an optional determiner, followed by zero or more adjectives of any type, followed by one or more nouns of any type. \n",
    "\n",
    "To find the chunk structure for a given sentence, the RegexpParser chunker begins with\n",
    "a flat structure in which no tokens are chunked. The chunking rules are applied in turn,\n",
    "successively updating the chunk structure. Once all of the rules have been invoked, the\n",
    "resulting chunk structure is returned.\n",
    "\n",
    "The next example shows a simple chunk grammar consisting of two rules. The first rule\n",
    "matches an optional determiner or possessive pronoun, zero or more adjectives, then a noun. The second rule matches one or more proper nouns. We also define an example\n",
    "sentence to be chunked , and run the chunker on this input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|P.*>?<JJ>*<NN.*>+} # chunk determiner/possessive, adjectives and nouns\n",
    "    {<NNP>+} # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "example = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"),\n",
    "            (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
    "cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced7792",
   "metadata": {},
   "source": [
    "If a tag pattern matches at overlapping locations, the leftmost match takes precedence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf4493",
   "metadata": {},
   "source": [
    "Sometimes it is easier to define what we want to exclude from a chunk. We can define a chink to be a sequence of tokens that is not included in a chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf50aa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\" NP:\n",
    "    {<.*>+}        # Chunk everything\n",
    "    }<CC|.*DT|TO>?<\\.|,|VB.*>+<IN>?{  # Chink \n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a9f8b",
   "metadata": {},
   "source": [
    "As befits their intermediate status between tagging and parsing, chunk\n",
    "structures can be represented using either tags or trees. The most widespread file representation\n",
    "uses IOB tags. In this scheme, each token is tagged with one of three special\n",
    "chunk tags, I (inside), O (outside), or B (begin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9198cc2",
   "metadata": {},
   "source": [
    "#### Developing and Evaluating Chunkers\n",
    "Now you have a taste of what chunking does, but we haven’t explained how to evaluate\n",
    "chunkers. As usual, this requires a suitably annotated corpus. We begin by looking at\n",
    "the mechanics of converting IOB format into an NLTK tree, then at how this is done\n",
    "on a larger scale using a chunked corpus. We will see how to score the accuracy of a\n",
    "chunker relative to a corpus, then look at some more data-driven ways to search for\n",
    "NP chunks. Our focus throughout will be on expanding the coverage of a chunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it works like this \n",
    "print(cp.accuracy((result,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "38 / len(sentence) # 38 tokens have been correctly classified in terms of IOB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f42d5",
   "metadata": {},
   "source": [
    "Using the corpora module we can load the data `conll2000` that has been tagged\n",
    "then chunked using the IOB notation. The chunk categories provided in this corpus\n",
    "are NP, VP, and PP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d262a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "conll2000.chunked_sents('train.txt')[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conll2000.chunked_sents('train.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9a50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(\"\") # we are not providing any grammar!\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7e439",
   "metadata": {},
   "source": [
    "Now let’s try a naive regular expression chunker that\n",
    "looks for tags beginning with letters that are characteristic of noun phrase tags (e.g.,\n",
    "`CD` (cardinal number), `DT`, and `JJ`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f665731",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar) \n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d503f3",
   "metadata": {},
   "source": [
    "We can improve on it\n",
    "by adopting a more data-driven approach, where we use the training corpus to find the\n",
    "chunk tag (I, O, or B) that is most likely for each part-of-speech tag. In other words, we\n",
    "can build a chunker using a unigram tagger (two weeks ago). But rather than trying to\n",
    "determine the correct part-of-speech tag for each word, we are trying to determine the\n",
    "correct chunk tag, given each word’s part-of-speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] \\\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "        in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ecbea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.chunk.tree2conlltags(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print(unigram_chunker.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf537c",
   "metadata": {},
   "source": [
    "### Training Classifier-Based Chunkers\n",
    "Both the regular expression–based chunkers and the n-gram chunkers decide what\n",
    "chunks to create entirely based on part-of-speech tags. However, sometimes part-ofspeech\n",
    "tags are insufficient to determine how a sentence should be chunked. For example,\n",
    "consider the following two statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "Tree.fromstring(\"(S (NP Joey) sold (NP the farmer) (NP rice) .)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c782799",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = Tree.fromstring(\"(S (NP Joey) sold (NP the computer monitor) .)\")\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca52376",
   "metadata": {},
   "outputs": [],
   "source": [
    "unchunked = example.flatten()\n",
    "unchunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_chunker.parse(nltk.pos_tag(unchunked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_chunker.parse(nltk.pos_tag(Tree.fromstring(\"(S (NP Joey) sold (NP the farmer) (NP rice) .)\").flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bebceb5",
   "metadata": {},
   "source": [
    "One way that we can incorporate information about the content of words is to use a\n",
    "classifier-based tagger to chunk the sentence. Like the n-gram chunker considered in\n",
    "the previous section, this classifier-based chunker will work by assigning IOB tags to\n",
    "the words in a sentence, and then converting those tags to chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60db4c",
   "metadata": {},
   "source": [
    "### Cascaded Chunks\n",
    "\n",
    "So far, our chunk structures have been relatively flat. Trees consist of tagged tokens,\n",
    "optionally grouped under a chunk node such as NP. However, it is possible to build\n",
    "chunk structures of arbitrary depth, simply by creating a multistage chunk grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40532432",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|JJ>*<NN.*>+} # Chunk sequences of DT, JJ, NN (noun phrase)\n",
    "    PP: {<IN><NP>} # Chunk prepositions followed by NP (prepositional phrase)\n",
    "    VP: {<VB.*><NP|PP>+$} # Chunk verbs and their arguments (verb phrase)\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9274e",
   "metadata": {},
   "source": [
    "This solution is not perfect, as `NP` are needlessly split. We will refine our grammar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b6fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP: {<DT|JJ|NN.*>+} # Chunk sequences of DT, JJ, NN (noun phrase)\n",
    "    PP: {<IN><NP>} # Chunk prepositions followed by NP (prepositional phrase)\n",
    "    VP: {<VB.*><NP|PP>+$} # Chunk verbs and their arguments (verb phrase)\n",
    "    NP: {<NP|PP><CC>?<NP|PP>}\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar, loop = 2)\n",
    "cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de509c6",
   "metadata": {},
   "source": [
    "Recall: The left side takes precedence when assigning the chunks! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ffd89",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "Named entities are definite noun phrases that refer to specific types of individuals, such as organizations,\n",
    "persons, dates, and so on.\n",
    "\n",
    "| NAMED ENTITY | EXAMPLE | \n",
    "| ---- | ---- |\n",
    "| ORGANIZATION | Georgia-Pacific Corp., WHO |\n",
    "| PERSON | Eddy Bonte, President Obama |\n",
    "| LOCATION | Murray River, Mount Everest |\n",
    "| DATE | June, 2008-06-29 |\n",
    "| TIME | two fifty a m, 1:30 p.m. |\n",
    "| FACILITY | Washington Monument, Stonehenge |\n",
    "| GEO-POLITICAL ENTITIES | South East Asia, Midlothian |\n",
    "\n",
    "The goal of a named entity recognition (NER) system is to identify textual mentions\n",
    "of the named entities. This can be broken down into two subtasks: identifying\n",
    "the boundaries of the NE, and identifying its type. \n",
    "\n",
    "How do we go about identifying named entities? One option would be to look up each\n",
    "word in an appropriate list of names. However, this is prone to errors caused by the fact that many named entity terms\n",
    "are ambiguous.\n",
    "\n",
    "Named entity recognition is a task that is well suited to the type of classifier-based\n",
    "approach that we saw for noun phrase chunking. In particular, we can build a tagger\n",
    "that labels each word in a sentence using the IOB format, where chunks are labeled by\n",
    "their appropriate type.\n",
    "\n",
    "NLTK provides a classifier that has already been trained to recognize named entities,\n",
    "accessed with the function `nltk.ne_chunk()`. If we set the parameter `binary=True`,\n",
    "then named entities are just tagged as `NE`; otherwise, the classifier adds category labels\n",
    "such as `PERSON`, `ORGANIZATION`, and `GPE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aae1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html as lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get('https://plato.stanford.edu/entries/liberalism-latin-america/')\n",
    "html=lx.fromstring(r.text)\n",
    "d=\" \".join(html.xpath('//div[@id=\"aueditable\"]//p//text()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b875770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "def preprocess(document):\n",
    "    document = re.sub(\"\\s+\", \" \", document)\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d928b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=preprocess(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120e809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a27e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[nltk.ne_chunk(sentence) for sentence in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e0aa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sent in t:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\"):\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b1cf22",
   "metadata": {},
   "source": [
    "### Relation Extraction\n",
    "\n",
    "Once named entities have been identified in a text, we then want to extract the relations\n",
    "that exist between them. We will typically be looking for relations\n",
    "between specified types of named entity. One way of approaching this task is to initially\n",
    "look for all triples of the form `(X, α, Y)`, where `X` and `Y` are named entities of the required\n",
    "types, and `α` is the string of words that intervenes between `X` and `Y`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
