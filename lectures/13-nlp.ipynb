{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff704263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA 141B Data & Web Technologies for Data Analysis\n",
    "\n",
    "### Lecture 13, 2/22/23, Natural language processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720fbd5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    " - Homework due tomorrow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb3382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Today's topics\n",
    "- Natural Language Processing\n",
    "     - Standardizing Text\n",
    "     - Feature extraction\n",
    "         - Term frequencies\n",
    "         - One-hot encoding\n",
    "         - Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190afa7",
   "metadata": {},
   "source": [
    "### Ressources\n",
    "- [Natural Language Processing with Python][nlpp], chapters 1-3. Beware: the print version is for Python 2.\n",
    "- [Scikit-Learn Documentation][skl], especially the section about [Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "\n",
    "[PDSH]: https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "[ProGit]: https://git-scm.com/book/\n",
    "[nlpp]: https://www.nltk.org/book/\n",
    "[atap]: https://search.library.ucdavis.edu/primo-explore/fulldisplay?docid=01UCD_ALMA51320822340003126&context=L&vid=01UCD_V1&search_scope=everything_scope&tab=default_tab&lang=en_US\n",
    "[skl]: https://scikit-learn.org/stable/documentation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f94d8",
   "metadata": {},
   "source": [
    "### Standardizing Text\n",
    "\n",
    "We standardize numerical data in order to make fair comparisons, comparisons that are not influenced by the location and scale of the data. Similarly, you can standardize text (tokens) to make sure comparisons are fair and accurate.\n",
    "\n",
    "For example, `\"Cat\"` and `\"cat\"` are the same word even though they're different tokens. Converting all characters to lowercase is one way to standardize a document.\n",
    "\n",
    "Some common standardization techniques for text are:\n",
    "\n",
    "* Lowercasing\n",
    "* Stemming: Use patterns to remove prefixes and suffixes from words.\n",
    "* Lemmatiziation: Look up each token in a dictionary and replace it with a root word. Similar to stemming, but more accurate.\n",
    "* Stopword Removal: Remove tokens that don't contribute meaning. For example, \"the\" is meaningless on its own.\n",
    "* Identifying Outliers: Identify and possibly remove non-standard \"words\" like numbers, mispellings, code, etc...\n",
    "\n",
    "How and whether you should standardize a document or corpus depends on what kind of analysis you want to do. There is no formula; you must think carefully and experiment to determine which standardization techniques work best for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92498eb3",
   "metadata": {},
   "source": [
    "#### Lowercasing\n",
    "\n",
    "You can use Python's string methods for simple text transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc71198",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter.lower()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f02f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter.upper()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b650aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = re.findall(r\"\\w+\", chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb972c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = [w.lower() for w in words] # lower and upper\n",
    "lower[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85ffc3",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "_Stemming_ runs an algorithm on each token to remove affixes (prefixes and suffixes). The result is called a _stem_.\n",
    "\n",
    "Stemming is useful if you want to ignore affixes.\n",
    "\n",
    "For example, most English verbs use suffixes to mark the tense. We write \"They fish\" (present) and \"They fished\" (past). Without any standardization, the tokens \"fish\" and \"fished\" would be treated as separate words. Stemming converts both tokens to the common stem \"fish\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5fb85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[nltk.PorterStemmer().stem(w) for w in words][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78092c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.PorterStemmer().stem(\"whales\"))\n",
    "print(nltk.PorterStemmer().stem(\"whaling\"))\n",
    "print(nltk.PorterStemmer().stem(\"whalebone\"))\n",
    "print(nltk.PorterStemmer().stem(\"narwhales\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74f0e6",
   "metadata": {},
   "source": [
    "Stemmers use a sequence of rules to determine the stem for each token, but natural languages are full of special cases and exceptions. So as you can see in the example above, some stems are not words , and sometimes tokens that seem like they should have the same stem don't.\n",
    "\n",
    "Several different stemmers are provided in the `nltk.stem` submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc949c",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "_Lemmatization_ looks up each token in a dictionary to find a root word, or _lemma_.\n",
    "\n",
    "Lemmatization serves the same purpose as stemming. Lemmatization is more accurate, but requires a dictionary and usually takes longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad88c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize(\"whales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize(\"whaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17885c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize(\"whaling\", \"v\") #this is a verb - it should be lemmatized to 'whale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize(\"whalebone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc390be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize(\"narwhales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28eced",
   "metadata": {},
   "source": [
    "The WordNet lemmatizer requires part of speech information in order to lemmatize words. You can get approximate part of speech information with __nltk__'s `pos_tag()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0997f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag([\"whaled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae88f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag([\"whaling\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdce05",
   "metadata": {},
   "source": [
    "NLTK POS Tags are [Brown POS tags][brown]\n",
    "\n",
    "[brown]: https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82a6f9d",
   "metadata": {},
   "source": [
    "#### Foreign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = SnowballStemmer('french')\n",
    "\n",
    "sent = \"En mathématiques, une fonction càdlàg (continue à droite, limite à gauche) est ...\"\n",
    "nltk.word_tokenize(sent)\n",
    "\n",
    "nltk.pos_tag([fr.stem(word) for word in nltk.word_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be58789",
   "metadata": {},
   "outputs": [],
   "source": [
    "moby_tags = nltk.pos_tag(words)\n",
    "moby_tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd0c9f",
   "metadata": {},
   "source": [
    "The `nltk.stem` submodule also provides several different lemmatizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62368977",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "_Stopwords_ are words that appear frequently but don't add meaning.\n",
    "\n",
    "In English, \"the\", \"a\", and \"at\" are examples. However, exactly which words are stopwords depends on your analysis. Words that are meaningless in one analysis might be very important in others.\n",
    "\n",
    "You can filter out stopwords with a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"the\", \"a\", \"and\", \"or\", \"in\", \"by\"]\n",
    "[w for w in words if w not in stopwords][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8ef17",
   "metadata": {},
   "source": [
    "__nltk__ also provides a stopwords corpus that contains common stopwords for several languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "[w for w in words if w not in stopwords][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecc095",
   "metadata": {},
   "source": [
    "### Feature Engineering for Natural Language Data\n",
    "\n",
    "Most statistical techniques take numbers as input. You may have already noticed this when working with categorical data. We can't compute the mean, median, standard deviation, or z-score if the observations aren't numbers. While we can fit linear models, it takes extra work because we have to create, or _engineer_, indicator variables.\n",
    "\n",
    "We face the same problem with natural language data. We need to _quantify_ documents, or turn them into numbers, so that we can use a wider variety of statistical techniques. We can do this by engineering features from our documents.\n",
    "\n",
    "So: what kinds of features can we create for language data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cfd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20e25d",
   "metadata": {},
   "source": [
    "#### Term Frequencies\n",
    "\n",
    "One solution is to extend the idea of frequency analysis. We used frequency analysis to study individual documents, but what if we compute the word frequencies for every document in our corpus, and use those frequencies as features?\n",
    "\n",
    "Let's try this for a small corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31581b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"The cat saw the dog was angry at the other cat.\", \n",
    "          \"The dog saw the cat was angry at the other cat.\", \n",
    "          \"The canary saw the iguana was sad.\"]\n",
    "\n",
    "def get_freq_doc(doc):\n",
    "    words = (w.lower() for w in nltk.word_tokenize(doc))\n",
    "    words = (w for w in words if w not in [\"the\", \"a\", \"an\", \"at\", 'other', \".\"] and w.isalnum())\n",
    "    return nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function to get frequency for each word\n",
    "df = pd.DataFrame([get_freq_doc(doc) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "df = df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a888a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The isalnum() method returns True if all characters in the string are alphanumeric\n",
    "\"dog?2\".isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [re.findall(r\"\\w+\", chapter) for chapter in chapters]\n",
    "words.pop(0)\n",
    "words = [w for l in words for w in l] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ff74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8912835",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w.lower() for w in words]\n",
    "#words = [w for w in words if w.isalnum()]\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93886e",
   "metadata": {},
   "source": [
    "`fq` will give the frequencies for each word, see [here](https://tedboy.github.io/nlps/generated/generated/nltk.FreqDist.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fq = nltk.FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c660767",
   "metadata": {},
   "outputs": [],
   "source": [
    "fq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53fca11",
   "metadata": {},
   "source": [
    "Frequency distribution objects have a few methods to provide summary information.\n",
    "\n",
    "The `.most_common()` method returns the most common tokens and their frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "fq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb40bf",
   "metadata": {},
   "source": [
    "A _hapax_ is a token that only occurs once within a document. The `.hapaxes()` method returns the hapaxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ad4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fq.hapaxes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3562449",
   "metadata": {},
   "source": [
    "The `.plot()` method displays a plot of word frequencies, sorted from most to least frequent word.\n",
    "\n",
    "The first parameter controls how many words to display. The second parameter controls whether the plot is cummulative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c122b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e55de4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fq.plot(40, cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad1c66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fq.plot(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5c722",
   "metadata": {},
   "source": [
    "Consider [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law): When the elements of a set - for example, the words of a text - are ordered by their frequency, the probability $p$ of their occurrence is inversely proportional to the place $n$ on the frequency list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8885eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logFreq = [np.log(i[1]) for i in fq.most_common(2000)]\n",
    "logRank = [np.log(1 + i) for i in range(0,2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd56439",
   "metadata": {},
   "outputs": [],
   "source": [
    "logTheo = [np.log(1/(1 + i)) for i in range(0,2000)] + logFreq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac80864",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(\n",
    "p9.ggplot() + p9.theme_minimal() + \n",
    "    p9.geom_line(p9.aes(x='logRank', y='logFreq')) + \n",
    "    p9.geom_line(p9.aes(x='logRank', y='logTheo'), color = 'red') \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276e6d2",
   "metadata": {},
   "source": [
    "Notice that when we use term frequencies as features, we lose information about the order of the words in each document.\n",
    "\n",
    "The first and second document contain the same words, but in different orders. The word frequency features for these two documents are identical.\n",
    "\n",
    "The __scikit-learn__ package provides functions to help with feature engineering. The `sklearn.feature_extraction.text` submodule is specifically for extracting features from text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9aacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e568068",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(tokenizer = nltk.word_tokenize)\n",
    "freq = vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d8903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .todense() convert sparse matrix to a dense matrix\n",
    "# Don't do this for a really large matrix!\n",
    "freq.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146dbd6",
   "metadata": {},
   "source": [
    "Use the `.get_feature_names_out()` method to see which term each column corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce7988",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea08b21a",
   "metadata": {},
   "source": [
    "One problem with term frequencies is that some terms have high frequencies simply because they appear frequently in the language. These terms can cause documents to appear similar even if they are otherwise different.\n",
    "\n",
    "While removing stopwords takes care of some high-frequency words, there may also be high-frequency words that have meaning and need to be kept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61d696",
   "metadata": {},
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "We can avoid emphasis on high-frequency words by ignoring frequency altogether. Instead, we can create indicator variables for individual words. The indicator is 1 if the word appears in the document, and 0 otherwise.\n",
    "\n",
    "In machine learning, an indicator variable is also called a _one-hot encoding_.\n",
    "\n",
    "The `sklearn.preprocessing` submodule of __scikit-learn__ provides a function for one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f914d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "help(Binarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0617495",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(freq > 0).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer()\n",
    "ohot = binarizer.fit_transform(freq)\n",
    "ohot.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c65d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a65f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0582acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af9470",
   "metadata": {},
   "source": [
    "As with term frequencies, we lose information about the order of the words in the document.\n",
    "\n",
    "One-hot encoding as an extreme transformation: every term is equally important. This means terms that are relatively rare or unique still might be underemphasized (this is also a problem for term frequencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b2639",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cff98d",
   "metadata": {},
   "source": [
    "_Term frequency-inverse document frequency_ (tf-idf) statistics put terms on approximately the same scale while also emphasizing relatively rare terms. There are [several different tf-idf statistics](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "The _smoothed tf-idf_, for a term $t$ and document $d$, is given by:\n",
    "\n",
    "$$\n",
    "\\operatorname{tf-idf}(t, d) = \\operatorname{tf}(t, d) \\cdot \\log \\left( \\frac{N}{1 + n_t} \\right)\n",
    "$$\n",
    "\n",
    "where $N$ is the total number of documents and $n_t$ is the number of documents that contain $t$.\n",
    "\n",
    "The `sklearn.feature_extraction.text` submodule of __scikit-learn__ provides a function for computing tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "help(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(tokenizer = nltk.word_tokenize) \n",
    "tfidf = vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a00ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 / 12) * np.log(3 / ( 1 + 3)) # '.' appears once in all 12 words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595ce82",
   "metadata": {},
   "source": [
    "In long documents or documents with many high-frequency terms, we can further reduce the emphasis on these terms by taking the logarithm of the term frequency. To do this, set `sublinear_tf = True` in the `TfidfVectorizer()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea08f8",
   "metadata": {},
   "source": [
    "## The Bag-of-words Model\n",
    "\n",
    "The one-hot encoding, term frequencies, and TF-IDF scores all ignore word order.\n",
    "\n",
    "The _bag-of-words model_ assumes that the order of words in a document doesn't matter. Imagine taking the words in each document and dumping them into a bag, where they get all mixed up. Note that in this case \"model\" means a way of thinking about a problem, not a statistical model.\n",
    "\n",
    "While the order of words in a document might seem important, the bag-of-words model is surprisingly useful. The bag-of-words model is a good place to start if you want to use statistical methods on language data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5742107",
   "metadata": {},
   "source": [
    "## Measuring Similarity\n",
    "\n",
    "We can measure the _similarity_ of two documents by computing the distance between their term frequency vectors. There are many different ways we can measure distance and similarity:\n",
    "\n",
    "* Minkowski distance, a family of distances that includes Euclidean distance ($\\ell_2$-norm) and Manhattan distance ($\\ell_1$-norm). \n",
    " * $\\ell_2$-norm, $\\|a - b \\|_2 = \\sqrt{\\sum_{i=1}^n (a_i - b_i)^2}$\n",
    " * $\\ell_1$-norm, $\\|a -b\\|_1 = \\sum_{i=1}^n |a_i - b_i|$\n",
    "\n",
    "* $\\ell_\\infty$-norm, $\\|a-b\\|_\\infty = \\max_{1\\leq i\\leq n} |a_i - b_i|$\n",
    "\n",
    "    * Relation between those norms: $\\|\\cdot\\|_1$ $\\geq$ $\\|\\cdot\\|_2$ $\\geq$ $\\cdots$ $\\geq$ $\\|\\cdot\\|_\\infty$\n",
    "\n",
    "* Mahalanobis distance, the Euclidean distance between z-scores.\n",
    "* Cosine similarity, the cosine of the angle between two vectors. See [here](https://stats.stackexchange.com/a/235676/29695) for an explanation of how cosine similarity is related to correlation. Note that the range of cosine is $[-1, 1]$ and $\\cos(0) = 1$, so vectors that are close together will have a cosine similarity close to 1, not 0.\n",
    "* And others...\n",
    "\n",
    "Cosine similarity often works well for language data. The cosine similarity between two vectors $a$ and $b$ is defined as:\n",
    "\n",
    "$$\n",
    "\\frac{a'b}{\\Vert a \\Vert_2 \\Vert b \\Vert_2}.\n",
    "$$\n",
    "\n",
    "The `TfidfVectorizer()` function already divides the returned tf-idf vectors by their Euclidean norms, so we can compute cosine similarity as a simple dot product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02574cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(tfidf @ tfidf.T).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec286e9",
   "metadata": {},
   "source": [
    "Part of the reason that cosine similarity is a good measure in NLP is that cosine similarity, like correlation, is not affected by the scale of the vector elements. For vectors that contain term frequencies (or functions of term frequencies), this means that the length of the original documents will not affect whether or not they are similar -- only their word content will."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb387b86",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3e878",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "- Standardize text first\n",
    "- Engineer features depending on priorities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2b12ec50b0a525a62abe739e766b0c808eccd181e3f804cedbbca00f4d5b392"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
